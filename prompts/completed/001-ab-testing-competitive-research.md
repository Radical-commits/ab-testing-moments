<research_objective>
Research how 3-5 major marketing automation and CX orchestration platforms implement A/B testing for customer engagement campaigns. This research will inform Infobip Moments product team's implementation strategy and support strategic decision-making.

Thoroughly explore multiple sources and consider various perspectives to identify prevailing approaches, best practices, and differentiated capabilities across the competitive landscape.
</research_objective>

<context>
Infobip Moments (https://www.infobip.com/moments) is a marketing automation and customer experience orchestration platform. The product team needs to understand how competitors approach A/B testing to make informed decisions about their own implementation strategy.

This research will be shared with internal stakeholders including product managers, engineering leads, and potentially executive leadership, so findings must be credible, well-sourced, and actionable.
</context>

<competitors_to_analyze>
Focus on 3-5 major competitors in the marketing automation and CX orchestration space. Prioritize market leaders such as:
- Salesforce Marketing Cloud
- Braze
- Adobe Campaign
- Iterable
- MoEngage
- Klaviyo
- Customer.io

Select the most relevant competitors based on feature parity with Infobip Moments and market presence.
</competitors_to_analyze>

<research_dimensions>
For each competitor, analyze A/B testing capabilities across three dimensions:

1. **Implementation Patterns & Technical Architecture**
   - How are A/B tests configured and executed within campaigns?
   - What technical approaches do they use (server-side, client-side, hybrid)?
   - How do tests integrate with campaign workflows and journey orchestration?
   - What test types are supported (subject lines, content, timing, channels, entire journeys)?
   - How do they handle multi-variant testing (A/B/n)?
   - What APIs or programmatic access do they provide for testing?

2. **User Experience & Workflow Design**
   - How do marketers create and manage A/B tests?
   - What is the setup workflow (number of steps, complexity level)?
   - How is test configuration presented visually?
   - How do they handle test audience segmentation and allocation?
   - What level of technical knowledge is required?
   - How are test results presented and visualized?

3. **Statistical Methodology & Analytics**
   - How do they calculate statistical significance?
   - What metrics can be tested (opens, clicks, conversions, revenue, custom events)?
   - How do they determine sample sizes and test duration?
   - What confidence levels do they use?
   - How do they handle early stopping or automatic winner selection?
   - What reporting and analytics are provided during and after tests?
</research_dimensions>

<research_process>
1. For each competitor, gather information from:
   - Official product documentation and help centers
   - Product marketing pages and feature descriptions
   - Demo videos and tutorials (YouTube, company channels)
   - User reviews on G2, Capterra, TrustRadius (look for A/B testing mentions)
   - Product comparison sites and analyst reports (if available)
   - Technical blog posts from the companies

2. Document specific features, capabilities, and approaches with source links

3. Identify patterns and common approaches across platforms

4. Note unique or differentiated capabilities that stand out

5. Look for gaps or limitations mentioned by users or in documentation
</research_process>

<deliverable_structure>
Create a comprehensive research report saved to: `./research/ab-testing-competitive-analysis.md`

Structure the report as follows:

## Executive Summary
- Key findings (3-5 bullet points)
- Prevailing approaches identified
- High-level recommendations

## Methodology
- Competitors analyzed
- Research sources used
- Date of research

## Competitor Analysis
For each competitor:

### [Competitor Name]
**Overview**: Brief description of the platform

**A/B Testing Implementation**:
- Implementation patterns & technical approach
- User experience & workflow
- Statistical methodology & analytics
- Notable strengths
- Notable limitations
- Source links

## Cross-Platform Patterns
Identify common approaches across competitors:
- Standard features present in most/all platforms
- Common technical patterns
- Typical user workflows
- Standard statistical approaches

## Differentiated Capabilities
Highlight unique or advanced capabilities that set certain platforms apart:
- Innovative approaches
- Advanced features
- Unique value propositions

## Gaps & Opportunities
- Features commonly requested but not widely available
- Limitations mentioned across platforms
- Potential areas for differentiation

## Strategic Recommendations
Based on the research, provide actionable recommendations for Infobip Moments:
1. **Core capabilities to implement** (table stakes features)
2. **Differentiating features to consider** (competitive advantages)
3. **Implementation approaches to evaluate** (technical strategy options)
4. **User experience principles to adopt** (workflow and design insights)

## Sources
- Complete list of all sources consulted with URLs
- Organized by competitor
</deliverable_structure>

<quality_standards>
- All findings must be supported by credible sources (official documentation, verified reviews, reputable analysis)
- Include direct links to sources for verification
- Distinguish between confirmed features and inferred capabilities
- Note the date of information since product capabilities change
- Be objective - acknowledge both strengths and weaknesses of each approach
- Focus on insights that will help Infobip make informed decisions
</quality_standards>

<success_criteria>
Before completing, verify:
- ✓ 3-5 major competitors analyzed in depth
- ✓ All three research dimensions covered for each competitor
- ✓ Common patterns identified across platforms
- ✓ Unique/differentiated capabilities highlighted
- ✓ Actionable strategic recommendations provided
- ✓ All findings supported by credible sources with links
- ✓ Report structured for mixed audience (executives + product team)
- ✓ Executive summary provides clear high-level insights
- ✓ Analysis is comprehensive enough to support decision-making
</success_criteria>

<constraints>
- Use clear, accessible language suitable for non-technical stakeholders
- Avoid marketing jargon - focus on concrete capabilities and approaches
- When explaining technical concepts, use plain language
- Make benefits and implications concrete and understandable
- Prioritize actionable insights over exhaustive feature lists
</constraints>
